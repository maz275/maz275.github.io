<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="description" content="Martin Abreu's personal website" />
  <meta name="generator" content="pandoc" />
  <meta name="keywords" content="Martin Abreu, Martin Abreu philosophy, Martin Abreu Zavaleta, philosophy of mind, lecture notes" />
  <meta name="author" content="Martín Abreu Zavaleta" />
  <title>Mart&#237;n Abreu</title>
<link rel="stylesheet" type="text/css" href="/style/style.css" media="screen"/>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53632616-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body>
<div id="main">
<!-- Header should go here-->
    <div id="links"></div>
      <div id="logo">
        <div id="logo_text">
          <h1>Mart&#237;n<span class="alternate_colour"> Abreu Zavaleta</span></h1>
        </div>
      </div>
      <div id="menubar">
        <ul id="menu">
          <!-- put class="tab_selected" in the li tag for the selected page - to highlight which page you're on -->
          <li><a href="/index.html">Home</a></li>
        <!--  <li><a href="research.html">Research</a></li> -->
          <li><a href="/cv.html">CV</a></li>
          <li class="tab_selected"><a href="/teaching.html">Teaching</a></li>
          <li><a href="/random.html">Random</a></li>
        </ul>
      </div>
  <!-- End of #header-->

<div id="site_content">
      <div id="contenta">
        <div id="header">
<h1 class="title">7. Introducing Functionalism</h1>
<!--<h2 class="author">Martín Abreu Zavaleta</h2>-->
<h3 class="date">June 5, 2014</h3>
</div>
<h1>Multiple realization</h1>
<p>Recall <em>Identity Theory</em>, which said that to have a certain mental state <span class="math"><em>m</em></span> is just to have a certain brain state <span class="math"><em>b</em></span>. Putnam (in “The nature of mental states&quot;) and others point out that this view has some important difficulties.</p>
<p>We may start with the observation that defenders of identity theory usually claim that the physical states that mental states are to be identified with are <em>physical-chemical</em> states. As Putnam observes, this seems too strong. Here’s why:</p>
<blockquote>
<p>He [the identity theorist] has to specify a physical-chemical state such that <em>any</em> organism (not just a mammal) is in pain if and only if (a) it possesses a brain of a suitable physical-chemical structure; and (b) its brain is in that physical-chemical state. This means that the physical-chemical state in question must be a possible state of a mammalian brain, a reptilian brain, a mollusk’s brain (octopuses are mollusk, and certainly feel pain), etc. At the same time, it must <em>not</em> be a possible (physically possible) state of the brain of any physically possible creature that cannot feel pain. (p. 77)</p>
</blockquote>
<p>Putnam is calling our attention to the apparent fact that such condition is very hard to satisfy: if we found aliens that behaved exactly like us, and yelled ‘ouch!’ every time we pinch them with needles, but are made of goo, identity theory tells us that they wouldn’t feel pain.</p>
<p>We don’t even need to go that far. Octopuses presumably feel pain, but their nervous systems seem to be very different from our own!<sup><a href="#fn1" class="footnoteRef" id="fnref1">1</a></sup> According to identity theory, if octopuses don’t have the physical-chemical kind of states that bring about pain in humans, that means they don’t experience pain.</p>
<p>But remember that identity theory is a general view about the nature of all mental states, not only pain. So, according to identity theory, unless something has the kind of physical-chemical states that bring about beliefs, desires, or even hunger in us (humans), it won’t have beliefs, desires, or hunger. This sounds like a bad consequence for the friends of identity theory.</p>
<p>This kind of objection is usually known as the argument from <em>multiple realizability</em>. We can define what it is for a state to be multiply realizable as follows:</p>
<dl>
<dt>Multiple-realizability:</dt>
<dd><p>A property <span class="math"><em>P</em></span> is multiply-realizable just in case it can be instantiated by different things in virtue of their having different properties.</p>
</dd>
</dl>
<p>For instance, the property of <em>being in pain</em> is multiply realizable because it can be instantiated in humans in virtue of the stimulation of C-fibers, in octopuses in virtue of the stimulation of O-fibers, in martians in virtue of the inflammation of M-nodes, et cetera. Because pain is multiply realizable, there need not be any particular physical property that everything must have in order to be in pain.</p>
<p>Most people believe that <em>mental states are multiply realizable</em>. It is because this thesis—the thesis that mental states are multiply realizable— is highly plausible that they came to formulate theories like <em>Functionalism</em>, which we will examine now. <strong>Question:</strong> Is behaviorism with the claim that mental states are multiply realizable?</p>
<h1>Functionalism</h1>
<p>We can generally describe functionalism as follows:</p>
<dl>
<dt>Functionalism:</dt>
<dd><p>Mental states are constituted by their causal role; that is, by their causal relations to other mental states, as well as their relations to inputs and outputs.</p>
</dd>
</dl>
<p>What does this mean? Let’s take a look at some things that we can identify by the roles they play. For instance, a water pump: anything that can move a fluid from one place to another in certain ways is a water pump. Or think of a piston: the job of a piston is to transfer the energy in a cylinder to a crankshaft by making a certain movement. Pistons are usually made out of metal, but at least in principle, there could be pistons made out of wood, plastic, or even rock (though perhaps all these pistons wouldn’t last long).</p>
<p>What makes a piston a piston is not the material of which it is made, but the role that it plays in the functioning of certain kinds of engines. Notice that in our characterization of a piston above, we didn’t only describe the movements that it makes. Instead, we described the relations that it has to other parts of the engine, like the cylinder or the crankshaft.</p>
<p>The idea of functionalism is that mental states can be defined in very much the way we defined a piston: by describing their functional roles in a given system. For instance, here is a very simplistic functional characterization of pain: being in pain is being in a state produced by sitting on a tack, that itself produces the state of being annoyed and the output of yelling ‘ouch!’ <strong>Question: How is this different from behaviorism?</strong></p>
<p>Over the course of the years, different philosophers have offered different methods for functionally characterizing a mental state—though note that these methods can be used to give functional characterizations of anything whatsoever! Because of its historical influence and because it was the earliest formal characterization, we will take a look at functional characterizations of mental states in terms of machine states.</p>
<h2 class="unnumbered">Putnam’s machine functionalism</h2>
<p>In his seminal paper “The nature of mental states” Putnam offers probably the first version of machine functionalism:</p>
<blockquote>
<p>The hypothesis that “being in pain is a functional state of the organism” may now be spelled out more exactly as follows:</p>
<ol>
<li><p>All organisms capable of feeling pain are Probabilistic Automata.</p></li>
<li><p>Every organism capable of feeling pain possesses at least one Description of a certain kind (i.e. being capable of feeling pain <em>is</em> possessing an appropriate kind of Functional Organization).</p></li>
<li><p>No organism capable of feeling pain possesses a decomposition into parts which separately possess Descriptions of the kind referred to in (2).</p></li>
<li><p>For every Description of the kind referred to in (2), there exists a subset of the sensory inputs such that an organism with that Description is in pain when and only when some of its sensory inputs are in that subset.</p></li>
</ol>
</blockquote>
<p>In order to understand what this means, we must know what probabilistic automata and descriptions are, in the sense relevant to Putnam’s view.</p>
<h2 class="unnumbered">Software, machine tables, and functional states</h2>
<p>We can define a machine by means of its machine table: a complete description of all the relations between its internal states with inputs, outputs, and other mental states. In order to illustrate the idea, it will be useful to introduce an abstract characterization of computing machines: <em>Turing machines</em>.</p>
<p>A Turing machine is made up four components:</p>
<ol>
<li><p>A tape divided into square, unbounded in both directions.</p></li>
<li><p>A head, positioned at one of the squares at any given time. It can scan and print.</p></li>
<li><p>A finite set of internal states.</p></li>
<li><p>A finite set of symbols (or alphabet).</p></li>
</ol>
<p>The machine operates according to the following general rules:</p>
<ol>
<li><p>At each time, the machine is in one of its internal states and scans the square where it’s positioned.</p></li>
<li><p>What the machine does at a given time is completely determined by its internal state at that time and the symbol its head is scanning at that time.</p></li>
<li><p>Depending on its internal state, the machine can do one of three things:</p>
<ol>
<li><p>The head erases a symbol and prints a symbol (possibly the same symbol it erased)</p></li>
<li><p>The head moves to the right, to the left, or stays in place.</p></li>
<li><p>The machine enters one of its internal states (possibly the same state it was already in).</p></li>
</ol></li>
</ol>
<p>A <em>machine table</em> is a complete set of instructions that defines a program. Every program can be defined by means of a machine table. Here is a machine table for a very simple addition program using a Turing machine:</p>
<p>| c | c | c | c | c | <strong>Input</strong> &amp; <strong>Present state</strong> &amp; <strong>Print</strong> &amp; <strong>Change to state</strong> &amp; <strong>Move</strong><br />1 &amp; Start &amp; 1 &amp; Start &amp; right<br /> &amp; A &amp; 1 &amp; A &amp; right<br /> &amp; B &amp; 0 &amp; Halt &amp; stay<br />0 &amp; Start &amp; 1 &amp; A &amp; right<br /> &amp; A &amp; 0 &amp; B &amp; left<br /></p>
<p>Note that the machine table specifies the following things:</p>
<ul>
<li><p>How the machine responds to input.</p></li>
<li><p>How the states of the machine are related to each other.</p></li>
<li><p>The conditions under which the machine will give certain output.</p></li>
</ul>
<p>With all these notions in place, we can offer a simple characterization of <strong>Putnam’s view</strong>: <em>to be capable of being in a certain mental state is to be describable by a particular kind of machine table, and to be in pain is to be in a certain state </em></p>
<p>What Putnam calls a <em>Description</em> is simply a very long sentence stating all the information that we can find in a machine table, and no more. For instance, a description of the machine characterized by the table above would say that the machine has four states, such that if the machine is in one of them (start) and it receives input 1, then it will remain in the same state and move to the right, if the machine is in another of them (A), and receives input 1, then it will remain in the same state and move to the right, and so on.</p>
<p>We almost have all we need to understand Putnam’s view! The only thing we’re missing is the notion of a <em>Probabilistic automaton</em>. You may have noticed that our machine table above leaves no room for chance: whenever the machine is in a given state and receives a certain input, it will determinately do a certain thing. Instead of doing this, we could have merely assigned a probability that the machine does a certain thing if it receives a certain input and is in a certain state.</p>
<p>For instance, we could have said that if the machine receives input 1 and is in state Start, then it will print 1 with a probability of .7 or print 0 with a probability of .3, it will remain in state Start with a probability of .6, or change to A with probability .3, or change to B with probability .1, and so on. The sum of the probabilities of the alternatives must always be 1!</p>
<p>So when Putnam says that all organisms capable of feeling pain are probabilistic automata, what he means is that they could be fully described in terms of a description like the one above. Moreover, only certain descriptions will characterize the kind of things that can feel pain (perhaps a very simple description won’t make something capable of feeling pain, but a more complex one will).</p>
<p>Condition 3 merely requires that the object that receives such description is not itself composed by objects that also receive that kind of description. For instance, if it turned out that a bee hive behaves in the ways described by the machine table that characterizes pain, and that the bees themselves are describable in those ways, Putnam doesn’t want to say that the bee itself is capable of experiencing pain!</p>
<h2 class="unnumbered">Advantages of this view</h2>
<p>Notice that the program characterized by the machine table above can be implemented in many different ways! It can be carried out by a machine modeled literally after a Turing machine, but it could also be carried out by a purely mechanical machine (like Babage’s Analytical Engine). It can even be carried out by a purely hydraulic machine, or by a machine made out of LEGOs. Thus, at least in principle, it seems that functionalism can account for the multiple realizability of mental states.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>For instance, two thirds of an octopus’s neurons are in its arms, rather than its brain.<a href="#fnref1">↩</a></p></li>
</ol>
</div>
<div>
<table style="width:100%">
<tr>
  <td><a href="/teaching/philmind/lecture6.html">Previous: Behaviorism</a></td>
  <td align="right"><a href="/teaching/philmind/lecture8.html">Next: More on machine functionalism</a></td>
</tr>
</table>
</div>
  </div>  <!--End of #contenta-->
   </div><!--End of #site_content-->
<div id="footer">
<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="/cclogo.png" /></a><br />All contents by <a xmlns:cc="http://creativecommons.org/ns#" href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#109;&#97;&#114;&#116;&#105;&#110;&#46;&#97;&#98;&#114;&#101;&#117;&#64;&#110;&#121;&#117;&#46;&#101;&#100;&#117;" property="cc:attributionName" rel="cc:attributionURL">&#109;&#97;&#114;&#116;&#105;&#110;&#46;&#97;&#98;&#114;&#101;&#117;&#64;&#110;&#121;&#117;&#46;&#101;&#100;&#117;</a>  
   </div>
  </div><!--End of main-->
</body>
</html>
