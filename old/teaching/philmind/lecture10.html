<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="description" content="Martin Abreu's personal website" />
  <meta name="generator" content="pandoc" />
  <meta name="keywords" content="Martin Abreu, Martin Abreu philosophy, Martin Abreu Zavaleta, philosophy of mind, lecture notes" />
  <meta name="author" content="Martín Abreu Zavaleta" />
  <meta name="date" content="2014-06-10" />
  <title>Mart&#237;n Abreu</title>
<link rel="stylesheet" type="text/css" href="/style/style.css" media="screen"/>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53632616-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body>
<div id="main">
<!-- Header should go here-->
    <div id="links"></div>
      <div id="logo">
        <div id="logo_text">
          <h1>Mart&#237;n<span class="alternate_colour"> Abreu Zavaleta</span></h1>
        </div>
      </div>
      <div id="menubar">
        <ul id="menu">
          <!-- put class="tab_selected" in the li tag for the selected page - to highlight which page you're on -->
          <li><a href="/index.html">Home</a></li>
        <!--  <li><a href="research.html">Research</a></li> -->
          <li><a href="/cv.html">CV</a></li>
          <li class="tab_selected"><a href="/teaching.html">Teaching</a></li>
          <li><a href="/random.html">Random</a></li>
        </ul>
      </div>
  <!-- End of #header-->

<div id="site_content">
      <div id="contenta">
        <div id="header">
<h1 class="title">10. Ned Block’s objections to functionalism</h1>
<!--<h2 class="author">Martín Abreu Zavaleta</h2>-->
<h3 class="date">June 10, 2014</h3>
</div>
<h1>Absent qualia</h1>
<p>Block invites us to consider the following two cases. In both cases, let’s suppose we have an appropriate specification of inputs and outputs:</p>
<dl>
<dt>Homunculi-Headed Robots:</dt>
<dd><p>“Imagine a body externally like a human body, say yours, but internally quite different. The neurons from sensory organs are connected to a bank of lights in a hollow cavity in the head. A set of buttons connects to the motor-output neurons. Inside the cavity resides a group of little men. Each has a very simple task: to implement a &quot;square&quot; of an adequate machine table that describes you. On one wall is a bulletin board on which is posted a state card, i.e., a card that bears a symbol designating one of the states specified in the machine table. Here is what the little men do: Suppose the posted card has a ’G’ on it... Suppose the light representing input I17 goes on. One of the G-men has the following as his sole task: when the card reads ’G’ and the I17 light goes on, he presses output button O191 and changes the state card to ’M’... In spite of the low level of intelligence required of each little man, the system as a whole manages to simulate you because the functional organization they have been trained to realize is yours...” (p. 278)</p>
</dd>
<dt>China brain:</dt>
<dd><p>Suppose we convert the government of China to functionalism... We provide each of the billion people in China (I chose China because it has a billion inhabitants) with a specially designed two-way radio that connects them in the appropriate way to other persons and to the artificial body mentioned in the previous example. We replace the little men with a radio transmitter and receiver connected to the input and output neurons. Instead of a bulletin board, we arrange to have letters displayed on a series of satellites placed so that they can be seen anywhere in China. Surely such a system is not physically impossible. It could be functionally equivalent to you for a short time, say an hour.” (p. 279)</p>
</dd>
</dl>
<p>In each case, functionalism is committed to saying that the whole system has whatever mental states you have. But there seems to be a <em>prima facie</em> doubt whether it has mental states, especially whether it has <em>qualitative mental states</em> (e.g. whether there is anything it is like for that system to see a red apple, or to taste an apricot).</p>
<p>Relying on intuitions to make substantive philosophical points is bad methodology, but Block adds that we have some arguments supporting our intuition:</p>
<ul>
<li><p>We have reason to disregard the intuition that brain-headed systems lack qualia, but we don’t have any reason to disregard the intuition that homunculi-headed systems lack qualia. This is so because we <strong>know</strong> that we are brain-headed systems and we have qualia.</p></li>
<li><p>“it is a highly plausible assumption that mental states are in the domain of psychology and/or neurophysiology, or at least that mentality depends crucially of psychological and/or neurophysiological processes and structures. But since the homunculi-headed Functional simulation of you is markedly unlike you neurophysiologically (insofar as it makes sense to speak of something with no neurons at all being neurophysiologically unlike anything) and since it need not be anything like you psychologically (that is, the information processing need not be remotely like yours), it is reasonable to doubt that it has mentality, even if it is Functionally equivalent to you&quot; (p. 196) Note that this seems to make it a better objection against commonsense functionalism than against psychofunctionalism.</p></li>
</ul>
<h2 class="unnumbered">The problem with Putnam’s response</h2>
<p>In his paper “The nature of mental states” Putnam claims that in order for something to have mentality, it can’t be that it has objects which themselves are mental as parts. One problem with this view is that it is <em>ad hoc</em>. A second problem is that it’s too strong. Let’s suppose for a second that there are very small beings that build spaceships the size of subatomic particles, and that those spaceships behave the way that Physics says subatomic particles behave. We may give a story explaining how a person comes to be composed entirely of those little spaceships, but we have no reason to suppose that such a person would be deprived of mentality. Block claims that one important difference between this case and the homunculi-head robot is that in the former, being composed of very little spaceships makes a difference only to the microphysics of a person, but not to her psychology. Not so with the latter.</p>
<h1>More problems with commonsense functionalism</h1>
<p>Most of the problems raised against commonsense functionalism point out that it seems to choose the wrong kind of theory to define mental states:</p>
<ul>
<li><p>Commonsense functionalism defines mental states in terms of supposedly platitudinous connections between behaviors and stimuli. For instance, it make take as input seeing a light or something like that, and as outputs things like moving arms and legs. But what about people who don’t have arms or legs? Block considers a case that makes the case seem more problematic:</p>
<blockquote>
<p>Perhaps the day will come when our brains will be periodically removed for cleaning. Imagine that this is done initially by treating neurons attaching the brain to the body with a chemical that allows them to stretch like rubber bands, so that no connections are disrupted. As technology advance, in order to avoid the inconvenience of one’s body being immobilized while one’s brain is serviced, brains are removed, the connections between brain and body being maintained by radio, while one goes about one’s business. After a few days, the customer returns and has the brain reinserted. Sometimes, however, people’s bodies are destroyed by accidents while their brains are being cleaned. If hooked up to input sense organs (but not output organs) these brains would exhibit none of the usual platitudinous connections between behavior and clusters of inputs and mental states. If, as seems plausible, these brains could have almost all the same mental states as we have, Functionalism is wrong. (p. 298)</p>
</blockquote></li>
<li><p>There may be two different sensations, A and B, which are not yet very well understood, so that there is no set of platitudes that differentiates them. According to commonsense functionalism, there is no mental difference between the two states, but this seems to be the wrong results.</p></li>
<li><p>Some of our intuitions about mental states may be false. Say that it is a platitude that pain usually causes yelling ’ouch!’, but after some research that it is not pain itself, but the different state of being annoyed, which causes the yelling. Then our commonsense psychological theory would be false, and so, on Lewis’s way of defining mental terms, no one would count as being in pain. This seems mistaken.</p></li>
</ul>
<h1>Inverted qualia</h1>
<p>We can suppose that there are two people, A and B, such that the objects that they both call green look to A the way objects we call green look to us, but to B the way objects we call red look to us. But we can further assume that those sensations in A and B play exactly the same causal roles, in which case functionalism would claim that they have exactly the same mental state. However, their mental states are different: one has an experience of green (A) and the other has an experience of red (B). In other words, whenever they see something that they both call green, they have different <em>qualia.</em></p>
<h1>Chauvinism vs. Liberalism</h1>
<p>Psychofunctionalism claims that in order for something to have a certain mental state, it must stand in the appropriate causal relations to whatever psychological events, states, processes and other entities actually obtain in us in whatever way such entities are causally related to one another. However, this entails that a lot of things that would intuitively have mental states would in fact have mental states. Block uses the following example:</p>
<blockquote>
<p>Suppose we meet Martians and find that they are roughly Functionally [that is, functionally like us as determined by commonsense psychology] (but not Psychofunctionally) equivalent to us. When we get to know Martians, we find them about as different from us as humans we know. We develop extensive cultural and commercial intercourse with [the Martians]. We study each other’s science and philosophy journals, go to each other’s movies, read each other’s novels, etc. Then Martian and Earthian psychologists compare notes, only to find that in underlying psychology, Martians and Earthians are very different... Imagine that what Martian and Earthian psychologists find when they compare notes is that Martians and Earthians differ as if they were the end products of maximally different design choices (compatible with rough functional equivalence in adults). Should we reject our assumption that Martians can enjoy our films, believe their own apparent scientific results, etc.?... Surely there are many ways of filling in the Martian/Earthian difference I sketched on which it would be perfectly clear that even if Martians behave differently from us on subtle psychological experiments, they nonetheless think, desire, enjoy, etc. To suppose otherwise would be crude human chauvinism. (Remember theories are chauvinist insofar as they falsely <em>deny</em> that systems have mental properties and liberal insofar as they falsely <em>attribute</em> mental properties.)(pp. 310-311)</p>
</blockquote>
<p>If Psychofunctionalism is true, then the Martians wouldn’t have the kinds of mental states that we do, and maybe they wouldn’t even have mental states! This seems to be the wrong consequence.</p>
<h1>Specifying inputs and outputs</h1>
<p>The commonsense functionalist specifies inputs and outputs in the same way as the behaviorist, which is in terms of sensory inputs and things like hand movements, utterances, and the like. This is chauvinist: it has the consequence that anything without hands, or without the ability to make utterances would be able to have mental states.</p>
<p>On the other hand, psychofunctionalism defines inputs and outputs in terms of neural activity. But then the only creatures capable of having such inputs and outputs will be the ones that are neurologically like us, or that have neurons in the first place. But what about creatures with no neurons, or creatures with different neural structures? According to psychofunctionalism, they wouldn’t have mental states either.</p>
<p>One way to solve this problem would be to define the inputs and outputs themselves in a functionalist fashion. That is, to give functional specifications for them just like we gave functional specifications for mental states. However, there is an obvious problem with this strategy:</p>
<blockquote>
<p>Economic systems have inputs and outputs, e.g., influx and outflux of credits and debits. And economic systems also have a rich variety of internal states, e.g., having a rate of increase of GNP equal to double the Prime Rate. It does not seem impossible that a wealthy sheik could gain control of the economy of a small country, e.g., Bolivia, and manipulate its financial system to make it functionally equivalent to a person, e.g., himself. If this seems implausible, remember that the economic states, inputs, and outputs designated by the sheik to correspond to his mental state, inputs, and outputs, need not be &quot;natural&quot; economic magnitudes [...] The mapping from psychological magnitudes to economic magnitudes could be as bizarre as the sheik requires. (p. 315)</p>
</blockquote>
<p>But it’s just very implausible that whatever the sheik does, it can make the economy of Bolivia have a mental life. So this new way of specifying inputs and outputs is too liberal.</p>
<div>
<table style="width:100%">
<tr>
  <td><a href="/teaching/philmind/lecture9.html">Previous: Lewis on psychophysical and theoretical identifications</a></td>
  <td align="right"><a href="/teaching/philmind/lecture11.html">Next: Lewis defends commonsense functionalism</a></td>
</tr>
</table>
</div>
  </div>  <!--End of #contenta-->
   </div><!--End of #site_content-->
 <div id="footer">
<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="/cclogo.png" /></a><br />All contents by <a xmlns:cc="http://creativecommons.org/ns#" href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#109;&#97;&#114;&#116;&#105;&#110;&#46;&#97;&#98;&#114;&#101;&#117;&#64;&#110;&#121;&#117;&#46;&#101;&#100;&#117;" property="cc:attributionName" rel="cc:attributionURL">&#109;&#97;&#114;&#116;&#105;&#110;&#46;&#97;&#98;&#114;&#101;&#117;&#64;&#110;&#121;&#117;&#46;&#101;&#100;&#117;</a>  
   </div>
  </div><!--End of main-->
</body>
</html>
