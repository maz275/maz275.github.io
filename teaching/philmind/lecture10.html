<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <title>Mart&#237;n Abreu</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Martin Abreu Zavaleta" />
  <meta name="desciption" content="Mart&#237;n Abreu's personal website" />
  <meta name="keywords"  content="Martin Abreu, Martin Abreu philosophy, Martin Abreu Zavaleta" />
  <meta name="Resource-type" content="Document" />
  <link rel="shortcut icon" type="image/x-icon" href="http://martinabreu.net/favicon.ico?v=005" />
  <link rel="stylesheet" media="screen and (min-width:700px)" type="text/css" href="/new.css" />
  <link rel="stylesheet" media="screen and (max-width:700px)" type="text/css" href="/style-mobile.css" />
  <!---  <link rel="stylesheet" media="screen and (max-width:700px) and (max-height:400px" type="text/css" href="/style-mobile.css" /> -->
  <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js" type="text/javascript"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-952PNT65JH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-952PNT65JH');
</script>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53632616-1', 'auto');
  ga('send', 'pageview');

</script>
</head>

<body>


<div id=logo>
       <div id="logo_text" class="center">
          <h1>Mart&#237;n<span class="alternate_color"> Abreu Zavaleta</span></h1>
       </div>
</div>

<nav>
<a href="#" id="menu-icon"></a>
    <ul id = "drop">
      <li><a href="/index.html#first">Home</a></li>
      <li><a href="/index.html#research">Research</a></li>
      <li><a href="#cv" onclick="window.open('http://www.martinabreu.net/cv.pdf')">CV</a></li>
	  <li><a class="active" href="/index.html#teaching">Teaching</a></li>
      <li><a href="/index.html#random">Random</a></li>
	</ul> 
</nav>

<div class="site_content">
<span id="cv"> </span>
   <div class="maketitle">
<h1 class="center">10. Ned Block&#8217;s objections to functionalism</h1>
<div class="center" ><span 
class="ptmr8t-x-x-120">Mart</span><span 
class="ptmr8t-x-x-120">&#237;n Abreu Zavaleta</span></div><br />
<div class="center" ><span 
class="ptmr8t-x-x-120">June 10, 2014</span></div>
   </div>
   <h2 class="sectionHead"><span class="titlemark">1    </span> <a 
 id="x1-10001"></a>Absent qualia</h2>
<!--l. 35--><p class="noindent" >Block invites us to consider the following two cases. In both cases, let&#8217;s suppose we have an appropriate
specification of inputs and outputs:
     <dl class="description"><dt class="description">
<span 
class="ptmb8t-x-x-109">Homunculi-Headed Robots:</span> </dt><dd 
class="description">&#8220;Imagine  a  body  externally  like  a  human  body,  say  yours,  but
     internally quite different. The neurons from sensory organs are connected to a bank of lights
     in a hollow cavity in the head. A set of buttons connects to the motor-output neurons. Inside
     the cavity resides a group of little men. Each has a very simple task: to implement a "square"
     of an adequate machine table that describes you. On one wall is a bulletin board on which is
     posted a state card, i.e., a card that bears a symbol designating one of the states specified in
     the machine table. Here is what the little men do: Suppose the posted card has a &#8217;G&#8217; on it...
     Suppose the light representing input I17 goes on. One of the G-men has the following as his
     sole task: when the card reads &#8217;G&#8217; and the I17 light goes on, he presses output button O191
     and changes the state card to &#8217;M&#8217;... In spite of the low level of intelligence required of each
     little man, the system as a whole manages to simulate you because the functional organization
     they have been trained to realize is yours...&#8221; (p. 278)
     </dd><dt class="description">
<span 
class="ptmb8t-x-x-109">China brain:</span> </dt><dd 
class="description">Suppose we convert the government of China to functionalism... We provide each of
     the billion people in China (I chose China because it has a billion inhabitants) with a specially
     designed two-way radio that connects them in the appropriate way to other persons and to
     the artificial body mentioned in the previous example. We replace the little men with a radio
     transmitter and receiver connected to the input and output neurons. Instead of a bulletin board,
     we arrange to have letters displayed on a series of satellites placed so that they can be seen
     anywhere in China. Surely such a system is not physically impossible. It could be functionally
     equivalent to you for a short time, say an hour.&#8221; (p. 279)</dd></dl>
                                                                                   
                                                                                   
<!--l. 40--><p class="noindent" >In each case, functionalism is committed to saying that the whole system has whatever mental states you have.
But there seems to be a <span 
class="ptmri8t-x-x-109">prima facie </span>doubt whether it has mental states, especially whether it has <span 
class="ptmri8t-x-x-109">qualitative</span>
<span 
class="ptmri8t-x-x-109">mental states </span>(e.g. whether there is anything it is like for that system to see a red apple, or to taste an
apricot).
<!--l. 42--><p class="indent" >   Relying on intuitions to make substantive philosophical points is bad methodology, but Block adds that
we have some arguments supporting our intuition:
     <ul class="itemize1">
     <li class="itemize">We have reason to disregard the intuition that brain-headed systems lack qualia, but we don&#8217;t
     have any reason to disregard the intuition that homunculi-headed systems lack qualia. This is
     so because we <span 
class="ptmb8t-x-x-109">know </span>that we are brain-headed systems and we have qualia.
     </li>
     <li class="itemize">&#8220;it  is  a  highly  plausible  assumption  that  mental  states  are  in  the  domain  of  psychology
     and/or neurophysiology, or at least that mentality depends crucially of psychological and/or
     neurophysiological  processes  and  structures.  But  since  the  homunculi-headed  Functional
     simulation of you is markedly unlike you neurophysiologically (insofar as it makes sense to
     speak of something with no neurons at all being neurophysiologically unlike anything) and
     since it need not be anything like you psychologically (that is, the information processing
     need not be remotely like yours), it is reasonable to doubt that it has mentality, even if it is
     Functionally equivalent to you" (p. 196) Note that this seems to make it a better objection
     against commonsense functionalism than against psychofunctionalism.</li></ul>
<!--l. 48--><p class="noindent" >
   <h4 class="likesubsectionHead"><a 
 id="x1-20001"></a>The problem with Putnam&#8217;s response</h4>
<!--l. 49--><p class="noindent" >In his paper &#8220;The nature of mental states&#8221; Putnam claims that in order for something to have mentality, it
can&#8217;t be that it has objects which themselves are mental as parts. One problem with this view is that it is <span 
class="ptmri8t-x-x-109">ad</span>
<span 
class="ptmri8t-x-x-109">hoc</span>. A second problem is that it&#8217;s too strong. Let&#8217;s suppose for a second that there are very small beings
that build spaceships the size of subatomic particles, and that those spaceships behave the way that Physics
says subatomic particles behave. We may give a story explaining how a person comes to be composed
entirely of those little spaceships, but we have no reason to suppose that such a person would be
deprived of mentality. Block claims that one important difference between this case and the
homunculi-head robot is that in the former, being composed of very little spaceships makes a
difference only to the microphysics of a person, but not to her psychology. Not so with the
latter.
<!--l. 51--><p class="noindent" >
   <h2 class="sectionHead"><span class="titlemark">2    </span> <a 
 id="x1-30002"></a>More problems with commonsense functionalism</h2>
<!--l. 52--><p class="noindent" >Most of the problems raised against commonsense functionalism point out that it seems to choose the
wrong kind of theory to define mental states:
     <ul class="itemize1">
     <li class="itemize">Commonsense functionalism defines mental states in terms of supposedly platitudinous
                                                                                   
                                                                                   
     connections between behaviors and stimuli. For instance, it make take as input seeing a light or
     something like that, and as outputs things like moving arms and legs. But what about people
     who don&#8217;t have arms or legs? Block considers a case that makes the case seem more
     problematic:
          <div class="quote">
          <!--l. 56--><p class="noindent" >Perhaps  the  day  will  come  when  our  brains  will  be  periodically  removed  for
          cleaning. Imagine that this is done initially by treating neurons attaching the brain
          to  the  body  with  a  chemical  that  allows  them  to  stretch  like  rubber  bands,  so
          that no connections are disrupted. As technology advance, in order to avoid the
          inconvenience  of  one&#8217;s  body  being  immobilized  while  one&#8217;s  brain  is  serviced,
          brains are removed, the connections between brain and body being maintained by
          radio, while one goes about one&#8217;s business. After a few days, the customer returns
          and has the brain reinserted. Sometimes, however, people&#8217;s bodies are destroyed by
          accidents while their brains are being cleaned. If hooked up to input sense organs
          (but not output organs) these brains would exhibit none of the usual platitudinous
          connections between behavior and clusters of inputs and mental states. If, as seems
          plausible, these brains could have almost all the same mental states as we have,
          Functionalism is wrong. (p. 298)</div>
     </li>
     <li class="itemize">There may be two different sensations, A and B, which are not yet very well understood, so that there
     is no set of platitudes that differentiates them. According to commonsense functionalism,
     there is no mental difference between the two states, but this seems to be the wrong
     results.
     </li>
     <li class="itemize">Some of our intuitions about mental states may be false. Say that it is a platitude that pain usually
     causes yelling &#8217;ouch!&#8217;, but after some research that it is not pain itself, but the different state of being
     annoyed, which causes the yelling. Then our commonsense psychological theory would be false, and
     so, on Lewis&#8217;s way of defining mental terms, no one would count as being in pain. This seems
     mistaken.</li></ul>
<!--l. 62--><p class="noindent" >
   <h2 class="sectionHead"><span class="titlemark">3    </span> <a 
 id="x1-40003"></a>Inverted qualia</h2>
<!--l. 63--><p class="noindent" >We can suppose that there are two people, A and B, such that the objects that they both call green look to A
the way objects we call green look to us, but to B the way objects we call red look to us. But we can
further assume that those sensations in A and B play exactly the same causal roles, in which case
functionalism would claim that they have exactly the same mental state. However, their mental states
are different: one has an experience of green (A) and the other has an experience of red (B).
In other words, whenever they see something that they both call green, they have different
<span 
class="ptmri8t-x-x-109">qualia.</span>
                                                                                   
                                                                                   
<!--l. 65--><p class="noindent" >
   <h2 class="sectionHead"><span class="titlemark">4    </span> <a 
 id="x1-50004"></a>Chauvinism vs. Liberalism</h2>
<!--l. 66--><p class="noindent" >Psychofunctionalism claims that in order for something to have a certain mental state, it must stand in the
appropriate causal relations to whatever psychological events, states, processes and other entities actually
obtain in us in whatever way such entities are causally related to one another. However, this entails that a
lot of things that would intuitively have mental states would in fact have mental states. Block uses the
following example:
     <div class="quote">
     <!--l. 68--><p class="noindent" >Suppose   we   meet   Martians   and   find   that   they   are   roughly   Functionally   [that
     is,   functionally   like   us   as   determined   by   commonsense   psychology]   (but   not
     Psychofunctionally) equivalent to us. When we get to know Martians, we find them
     about  as  different  from  us  as  humans  we  know.  We  develop  extensive  cultural
     and commercial intercourse with [the Martians]. We study each other&#8217;s science and
     philosophy  journals,  go  to  each  other&#8217;s  movies,  read  each  other&#8217;s  novels,  etc.  Then
     Martian  and  Earthian  psychologists  compare  notes,  only  to  find  that  in  underlying
     psychology,  Martians  and  Earthians  are  very  different...  Imagine  that  what  Martian
     and  Earthian  psychologists  find  when  they  compare  notes  is  that  Martians  and
     Earthians  differ  as  if  they  were  the  end  products  of  maximally  different  design
     choices  (compatible  with  rough  functional  equivalence  in  adults).  Should  we  reject
     our assumption that Martians can enjoy our films, believe their own apparent scientific
     results, etc.?... Surely there are many ways of filling in the Martian/Earthian difference
     I sketched on which it would be perfectly clear that even if Martians behave differently
     from  us  on  subtle  psychological  experiments,  they  nonetheless  think,  desire,  enjoy,
     etc. To suppose otherwise would be crude human chauvinism. (Remember theories are
     chauvinist insofar as they falsely <span 
class="ptmri8t-x-x-109">deny </span>that systems have mental properties and liberal
     insofar as they falsely <span 
class="ptmri8t-x-x-109">attribute </span>mental properties.)(pp. 310-311)</div>
<!--l. 70--><p class="noindent" >If Psychofunctionalism is true, then the Martians wouldn&#8217;t have the kinds of mental states that we do, and
maybe they wouldn&#8217;t even have mental states! This seems to be the wrong consequence.
<!--l. 72--><p class="noindent" >
   <h2 class="sectionHead"><span class="titlemark">5    </span> <a 
 id="x1-60005"></a>Specifying inputs and outputs</h2>
<!--l. 73--><p class="noindent" >The commonsense functionalist specifies inputs and outputs in the same way as the behaviorist, which is in
terms of sensory inputs and things like hand movements, utterances, and the like. This is chauvinist: it has
the consequence that anything without hands, or without the ability to make utterances would be able to
have mental states.
<!--l. 75--><p class="indent" >   On the other hand, psychofunctionalism defines inputs and outputs in terms of neural activity. But then
the only creatures capable of having such inputs and outputs will be the ones that are neurologically like us,
or that have neurons in the first place. But what about creatures with no neurons, or creatures with
different neural structures? According to psychofunctionalism, they wouldn&#8217;t have mental states
either.
                                                                                   
                                                                                   
<!--l. 77--><p class="indent" >   One way to solve this problem would be to define the inputs and outputs themselves in
a functionalist fashion. That is, to give functional specifications for them just like we gave
functional specifications for mental states. However, there is an obvious problem with this
strategy:
     <div class="quote">
     <!--l. 79--><p class="noindent" >Economic systems have inputs and outputs, e.g., influx and outflux of credits and debits.
     And economic systems also have a rich variety of internal states, e.g., having a rate
     of increase of GNP equal to double the Prime Rate. It does not seem impossible that
     a wealthy sheik could gain control of the economy of a small country, e.g., Bolivia,
     and  manipulate  its  financial  system  to  make  it  functionally  equivalent  to  a  person,
     e.g., himself. If this seems implausible, remember that the economic states, inputs, and
     outputs designated by the sheik to correspond to his mental state, inputs, and outputs,
     need  not  be  "natural"  economic  magnitudes  [...]  The  mapping  from  psychological
     magnitudes to economic magnitudes could be as bizarre as the sheik requires. (p. 315)</div>
<!--l. 81--><p class="noindent" >But it&#8217;s just very implausible that whatever the sheik does, it can make the economy of Bolivia have a mental
life. So this new way of specifying inputs and outputs is too liberal.  
  

                                                                                   
                                                                                   
                                                                                   


		<br />
		<br />
		<br />
		<div id="footer">
<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a>
		<p>All contents by <a xmlns:cc="http://creativecommons.org/ns#" href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#109;&#97;&#114;&#116;&#105;&#110;&#46;&#97;&#98;&#114;&#101;&#117;&#64;&#110;&#121;&#117;&#46;&#101;&#100;&#117;" property="cc:Mart&#237;n Abreu Zavaleta" rel="cc:martinabreu.net">&#109;&#97;&#114;&#116;&#105;&#110;&#46;&#97;&#98;&#114;&#101;&#117;&#64;&#110;&#121;&#117;&#46;&#101;&#100;&#117;</a> </p>
		</div>

   </div>

</body>
</html>
